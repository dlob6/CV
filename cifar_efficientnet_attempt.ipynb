{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai.vision import *\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>frog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>deer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       label\n",
       "0   1        frog\n",
       "1   2       truck\n",
       "2   3       truck\n",
       "3   4        deer\n",
       "4   5  automobile"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('trainLabels.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/1.png</td>\n",
       "      <td>frog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/2.png</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3.png</td>\n",
       "      <td>truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train/4.png</td>\n",
       "      <td>deer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train/5.png</td>\n",
       "      <td>automobile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id       label\n",
       "0  train/1.png        frog\n",
       "1  train/2.png       truck\n",
       "2  train/3.png       truck\n",
       "3  train/4.png        deer\n",
       "4  train/5.png  automobile"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TR_PATH = 'train/'\n",
    "img_enc = '.png'\n",
    "df_train['id'] = [f\"{TR_PATH}{o}{img_enc}\" for o in df_train['id'].tolist()]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GETTING IMAGE STATS\n",
    "\n",
    "n = len(df_train)\n",
    "glob_means = [0]*3\n",
    "glob_squares = [0]*3\n",
    "\n",
    "def im_to_np(i) : return np.array(Image.open(df_train['id'].iloc[i]))/255\n",
    "acc_squares = lambda x : np.sum(x**2)\n",
    "    \n",
    "# means per channel\n",
    "for im_ix in range(n) :\n",
    "    im = im_to_np(im_ix)\n",
    "    loc_means = [im[:,:,i].mean()/n for i in range(3)]\n",
    "    glob_means = [gm + lm for gm,lm in zip(glob_means,loc_means)]\n",
    "\n",
    "# accumulating sum of (x - mean(x))Â² per channel on all imgs\n",
    "for im_ix in range(n) :\n",
    "    im = im_to_np(im_ix)\n",
    "    loc_squares = [acc_squares(im[:,:,i]-glob_means[i]) for i in range(3)]\n",
    "    glob_squares = [gs + ls for gs,ls in zip(glob_squares,loc_squares)]\n",
    "    \n",
    "# standard deviations of channels\n",
    "glob_stds = [np.sqrt(o/(n*32*32)) for o in glob_squares]\n",
    "stats = [torch.Tensor(glob_means),torch.Tensor(glob_stds)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0.4914, 0.4822, 0.4465]), tensor([0.2470, 0.2435, 0.2616])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = (ImageDataBunch.from_df(path='.',df=df_train,label_col='label',\n",
    "                             ds_tfms = [flip_lr(),[]],valid_pct=0.3,bs=256)\n",
    "      .normalize(stats = stats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_lay(ni,nf,s=1,ks=3,padding=0, depthwise = False, act = True, zero_bn = False) :\n",
    "    \"\"\"Base conv layer, followed by Bn and an activation.\"\"\"\n",
    "    # 'ni': number of input channels\n",
    "    # 'nf': number of filters\n",
    "    # 's': stride\n",
    "    # 'ks': kernel size\n",
    "    if isinstance(ks,tuple) :\n",
    "        padding = (ks[0]//2,ks[1]//2)\n",
    "    else : padding = ks //2\n",
    "    bn = nn.BatchNorm2d(nf)\n",
    "    # Initialization trick from https://arxiv.org/pdf/1901.09321.pdf\n",
    "    # Not part of efficientnet AFAIK but better weight init should improve result anyway\n",
    "    nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    if act :\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(ni,nf,ks,stride=s,padding=padding,bias=False,groups= ni if depthwise else 1), \n",
    "            bn,\n",
    "            nn.ReLU6(inplace=True))\n",
    "    else :\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(ni,nf,ks,stride=s,padding=padding,bias=False,groups= ni if depthwise else 1), \n",
    "            bn)\n",
    "\n",
    "class mbconv_downsample(nn.Module):\n",
    "    # From https://arxiv.org/pdf/1801.04381.pdf\n",
    "    # Hopefully that's how they downsample in efficientnet, it's not told in the paper\n",
    "    def __init__(self, ni, nf, ks=(3,3)):\n",
    "        super().__init__()\n",
    "        self.conv1 = conv_lay(ni,nf,ks=1)\n",
    "        self.conv2 = conv_lay(nf,nf,ks=ks,s=2,depthwise=True)\n",
    "        self.conv3 = conv_lay(nf,nf,ks=1,act=False)\n",
    "        \n",
    "    def forward(self,x) :\n",
    "        return self.conv3(self.conv2(self.conv1(x)))\n",
    "\n",
    "class mbconv(nn.Module) :\n",
    "    \"\"\"Base inverted mobile resnet block\"\"\"\n",
    "    def __init__(self,ni,nf,exp=6, ks=(3,3)) :\n",
    "        super().__init__()\n",
    "        self.ni = ni\n",
    "        self.nf = nf\n",
    "        self.conv1 = conv_lay(ni,nf,ks=1)\n",
    "        self.conv2 = conv_lay(nf,nf*exp,ks=ks,depthwise=True)\n",
    "        self.conv3 = conv_lay(nf*exp,nf,ks=1,act=False, zero_bn = True)\n",
    "        if ni != nf: self.conv4 = conv_lay(ni,nf,ks=1,act=False)\n",
    "        \n",
    "    def forward(self,x) :\n",
    "        return self.conv3(self.conv2(self.conv1(x))) + self.conv4(x) if self.ni != self.nf else x\n",
    "\n",
    "    \n",
    "# MISSING: Squeeze and exication optimization, probably from  \n",
    "# https://arxiv.org/pdf/1709.01507.pdf\n",
    "# Should be added to the basic mbconv, as per comment in \n",
    "# https://arxiv.org/pdf/1905.11946.pdf below table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see table 1 in https://arxiv.org/pdf/1905.11946.pdf\n",
    "filters = [32,16,24,40,80,112,192,320]\n",
    "downsample_blocks = [1,3,6] # [1,2,3,6] for imagenet\n",
    "\n",
    "# Copy pasted from https://github.com/fastai/fastai/blob/master/fastai/vision/models/xresnet.py\n",
    "# Sort of fixup initialization from https://arxiv.org/pdf/1901.09321.pdf but using Batchnorm\n",
    "# Not part of efficientnet AFAIK but better weight init should improve result anyway\n",
    "def init_cnn(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn(l)\n",
    "\n",
    "class nnet(nn.Module) :\n",
    "    \"\"\"Base resnet-like model\"\"\"\n",
    "    def make_block(self,ni,nf,n,exp,ks=(3,3),s=1,downsample=False) :\n",
    "        \"\"\"conv + n*mbconv3\"\"\"\n",
    "        l = [mbconv(ni,nf,exp,ks)]\n",
    "        l += [mbconv(nf,nf,exp,ks) for _ in range(n-1)]\n",
    "        if downsample :\n",
    "            print(downsample)\n",
    "            l = l[:-1] + [mbconv_downsample(nf,nf)]\n",
    "        return l\n",
    "    \n",
    "    def __init__(self,blocks,n_classes=10):\n",
    "        super().__init__()\n",
    "        layers = [conv_lay(3,filters[0],s=2)]\n",
    "        for ix,n in enumerate(blocks) :\n",
    "            exp = 1 if ix == 0 else 6\n",
    "            ks = (5,5) if ix == 2 else (3,3)\n",
    "            downsample = ix in downsample_blocks\n",
    "            layers += self.make_block(filters[ix],filters[ix+1],n,exp = exp,\n",
    "                                      ks = ks, downsample = downsample)\n",
    "        layers += [nn.AdaptiveAvgPool2d(1)]\n",
    "        layers += [Flatten(),nn.Dropout(0.2),nn.Linear(filters[ix+1],n_classes)]\n",
    "        self.layers = nn.Sequential(*layers) #\n",
    "        self.c = n_classes\n",
    "        init_cnn(self)\n",
    "    \n",
    "    def forward(self,x) :\n",
    "        return self.layers(x) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network archi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nnet(\n",
       "  (layers): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace)\n",
       "    )\n",
       "    (1): mbconv(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): Sequential(\n",
       "        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): mbconv(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=24, bias=False)\n",
       "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): Sequential(\n",
       "        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): mbconv_downsample(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=24, bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(24, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): mbconv(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(24, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(40, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
       "        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): Sequential(\n",
       "        (0): Conv2d(24, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): mbconv(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(40, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(40, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=40, bias=False)\n",
       "        (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): mbconv(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (conv4): Sequential(\n",
       "        (0): Conv2d(40, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): mbconv(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(80, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=80, bias=False)\n",
       "        (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): mbconv_downsample(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=80, bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU6(inplace)\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): AdaptiveAvgPool2d(output_size=1)\n",
       "    (10): Flatten()\n",
       "    (11): Dropout(p=0.2)\n",
       "    (12): Linear(in_features=80, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blocks = [1,2,2,3]\n",
    "# Actual efficientnet has blocks = [1,2,2,3,3,4,1] but can't do that with cifar\n",
    "mod = nnet(blocks)\n",
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type)         Output Shape         Param #    Trainable \n",
       "======================================================================\n",
       "Conv2d               [32, 16, 16]         864        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [32, 16, 16]         64         True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [32, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [16, 16, 16]         512        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [16, 16, 16]         32         True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [16, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [16, 16, 16]         144        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [16, 16, 16]         32         True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [16, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [16, 16, 16]         256        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [16, 16, 16]         32         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [16, 16, 16]         512        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [16, 16, 16]         32         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [24, 16, 16]         384        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [24, 16, 16]         48         True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [24, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [144, 16, 16]        1,296      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [144, 16, 16]        288        True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [144, 16, 16]        0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [24, 16, 16]         3,456      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [24, 16, 16]         48         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [24, 16, 16]         384        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [24, 16, 16]         48         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [24, 16, 16]         576        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [24, 16, 16]         48         True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [24, 16, 16]         0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [24, 8, 8]           216        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [24, 8, 8]           48         True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [24, 8, 8]           0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [24, 8, 8]           576        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [24, 8, 8]           48         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [40, 8, 8]           960        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [40, 8, 8]           80         True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [40, 8, 8]           0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [240, 8, 8]          6,000      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [240, 8, 8]          480        True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [240, 8, 8]          0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [40, 8, 8]           9,600      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [40, 8, 8]           80         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [40, 8, 8]           960        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [40, 8, 8]           80         True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [80, 8, 8]           3,200      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [80, 8, 8]           160        True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [80, 8, 8]           0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [480, 8, 8]          4,320      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [480, 8, 8]          960        True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [480, 8, 8]          0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [80, 8, 8]           38,400     True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [80, 8, 8]           160        True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [80, 8, 8]           3,200      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [80, 8, 8]           160        True      \n",
       "______________________________________________________________________\n",
       "Conv2d               [80, 8, 8]           6,400      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [80, 8, 8]           160        True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [80, 8, 8]           0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [80, 4, 4]           720        True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [80, 4, 4]           160        True      \n",
       "______________________________________________________________________\n",
       "ReLU6                [80, 4, 4]           0          False     \n",
       "______________________________________________________________________\n",
       "Conv2d               [80, 4, 4]           6,400      True      \n",
       "______________________________________________________________________\n",
       "BatchNorm2d          [80, 4, 4]           160        True      \n",
       "______________________________________________________________________\n",
       "AdaptiveAvgPool2d    [80, 1, 1]           0          False     \n",
       "______________________________________________________________________\n",
       "Flatten              [80]                 0          False     \n",
       "______________________________________________________________________\n",
       "Dropout              [80]                 0          False     \n",
       "______________________________________________________________________\n",
       "Linear               [10]                 810        True      \n",
       "______________________________________________________________________\n",
       "\n",
       "Total params: 93,554\n",
       "Total trainable params: 93,554\n",
       "Total non-trainable params: 0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(db,mod,metrics=accuracy)#.mixup()\n",
    "learn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Total time: 10:23 <p><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.940101</td>\n",
       "      <td>1.820048</td>\n",
       "      <td>0.335467</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.722696</td>\n",
       "      <td>1.631568</td>\n",
       "      <td>0.406600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.558076</td>\n",
       "      <td>1.493928</td>\n",
       "      <td>0.454467</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.438359</td>\n",
       "      <td>1.395729</td>\n",
       "      <td>0.489533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.337923</td>\n",
       "      <td>1.333208</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.256165</td>\n",
       "      <td>1.287364</td>\n",
       "      <td>0.528133</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.170217</td>\n",
       "      <td>1.235951</td>\n",
       "      <td>0.552800</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.109211</td>\n",
       "      <td>1.173454</td>\n",
       "      <td>0.576867</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.046061</td>\n",
       "      <td>1.156415</td>\n",
       "      <td>0.585133</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.982230</td>\n",
       "      <td>1.100935</td>\n",
       "      <td>0.607933</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.925380</td>\n",
       "      <td>1.093699</td>\n",
       "      <td>0.622067</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.906553</td>\n",
       "      <td>1.089060</td>\n",
       "      <td>0.610133</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.850687</td>\n",
       "      <td>1.050641</td>\n",
       "      <td>0.632800</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.812492</td>\n",
       "      <td>1.099830</td>\n",
       "      <td>0.611867</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.783242</td>\n",
       "      <td>1.015558</td>\n",
       "      <td>0.648533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.755193</td>\n",
       "      <td>1.009049</td>\n",
       "      <td>0.662133</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.731768</td>\n",
       "      <td>0.977862</td>\n",
       "      <td>0.661933</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.712647</td>\n",
       "      <td>1.042627</td>\n",
       "      <td>0.652067</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.690026</td>\n",
       "      <td>0.935825</td>\n",
       "      <td>0.686533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.679537</td>\n",
       "      <td>0.934855</td>\n",
       "      <td>0.686067</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.656246</td>\n",
       "      <td>1.094706</td>\n",
       "      <td>0.659667</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.646660</td>\n",
       "      <td>0.996955</td>\n",
       "      <td>0.667533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.633649</td>\n",
       "      <td>0.877025</td>\n",
       "      <td>0.701400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.621600</td>\n",
       "      <td>0.960469</td>\n",
       "      <td>0.676000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.588313</td>\n",
       "      <td>0.857681</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.567631</td>\n",
       "      <td>0.842398</td>\n",
       "      <td>0.718533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.561911</td>\n",
       "      <td>1.007484</td>\n",
       "      <td>0.689533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.546845</td>\n",
       "      <td>0.827509</td>\n",
       "      <td>0.729600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.525594</td>\n",
       "      <td>0.897810</td>\n",
       "      <td>0.702133</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.505973</td>\n",
       "      <td>0.969955</td>\n",
       "      <td>0.698600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.486220</td>\n",
       "      <td>0.879852</td>\n",
       "      <td>0.720600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.467826</td>\n",
       "      <td>0.844568</td>\n",
       "      <td>0.729600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.446568</td>\n",
       "      <td>1.026856</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.425524</td>\n",
       "      <td>0.833980</td>\n",
       "      <td>0.731200</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.420041</td>\n",
       "      <td>0.821979</td>\n",
       "      <td>0.736667</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.408302</td>\n",
       "      <td>0.911261</td>\n",
       "      <td>0.720467</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.391283</td>\n",
       "      <td>0.976355</td>\n",
       "      <td>0.706267</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.363595</td>\n",
       "      <td>0.903441</td>\n",
       "      <td>0.732000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.346066</td>\n",
       "      <td>0.887066</td>\n",
       "      <td>0.736933</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.342649</td>\n",
       "      <td>0.890727</td>\n",
       "      <td>0.732533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.328534</td>\n",
       "      <td>1.068077</td>\n",
       "      <td>0.705400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.311638</td>\n",
       "      <td>0.911958</td>\n",
       "      <td>0.739667</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.303552</td>\n",
       "      <td>1.058840</td>\n",
       "      <td>0.719267</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.281260</td>\n",
       "      <td>0.927217</td>\n",
       "      <td>0.733933</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.275826</td>\n",
       "      <td>1.049423</td>\n",
       "      <td>0.726733</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.271610</td>\n",
       "      <td>0.976055</td>\n",
       "      <td>0.732933</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.241951</td>\n",
       "      <td>1.038806</td>\n",
       "      <td>0.732467</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.237197</td>\n",
       "      <td>1.071646</td>\n",
       "      <td>0.733667</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.224470</td>\n",
       "      <td>1.067920</td>\n",
       "      <td>0.740733</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.221771</td>\n",
       "      <td>1.001543</td>\n",
       "      <td>0.747067</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.205045</td>\n",
       "      <td>1.071174</td>\n",
       "      <td>0.739800</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.198611</td>\n",
       "      <td>1.141967</td>\n",
       "      <td>0.737133</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.176758</td>\n",
       "      <td>1.160550</td>\n",
       "      <td>0.739133</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.176731</td>\n",
       "      <td>1.124890</td>\n",
       "      <td>0.742067</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.167510</td>\n",
       "      <td>1.191638</td>\n",
       "      <td>0.736600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.153761</td>\n",
       "      <td>1.164738</td>\n",
       "      <td>0.744533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.152591</td>\n",
       "      <td>1.259907</td>\n",
       "      <td>0.728467</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.135177</td>\n",
       "      <td>1.197423</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.126310</td>\n",
       "      <td>1.166375</td>\n",
       "      <td>0.749733</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.115935</td>\n",
       "      <td>1.367293</td>\n",
       "      <td>0.735933</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.112208</td>\n",
       "      <td>1.238685</td>\n",
       "      <td>0.747933</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.112445</td>\n",
       "      <td>1.291809</td>\n",
       "      <td>0.743000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.095656</td>\n",
       "      <td>1.268705</td>\n",
       "      <td>0.750400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.085933</td>\n",
       "      <td>1.418681</td>\n",
       "      <td>0.741400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.084213</td>\n",
       "      <td>1.362364</td>\n",
       "      <td>0.749533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.076400</td>\n",
       "      <td>1.346086</td>\n",
       "      <td>0.754200</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.071467</td>\n",
       "      <td>1.439727</td>\n",
       "      <td>0.749667</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.059074</td>\n",
       "      <td>1.440579</td>\n",
       "      <td>0.755533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.055068</td>\n",
       "      <td>1.421750</td>\n",
       "      <td>0.756400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.047947</td>\n",
       "      <td>1.469374</td>\n",
       "      <td>0.750800</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.047501</td>\n",
       "      <td>1.474985</td>\n",
       "      <td>0.760533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.041245</td>\n",
       "      <td>1.500044</td>\n",
       "      <td>0.752800</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.039719</td>\n",
       "      <td>1.567061</td>\n",
       "      <td>0.752533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.032926</td>\n",
       "      <td>1.537673</td>\n",
       "      <td>0.760333</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.024363</td>\n",
       "      <td>1.578894</td>\n",
       "      <td>0.757333</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.024721</td>\n",
       "      <td>1.611447</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.024212</td>\n",
       "      <td>1.648086</td>\n",
       "      <td>0.754333</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.018068</td>\n",
       "      <td>1.644739</td>\n",
       "      <td>0.762267</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.016676</td>\n",
       "      <td>1.655277</td>\n",
       "      <td>0.758400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>1.679200</td>\n",
       "      <td>0.765867</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.008094</td>\n",
       "      <td>1.691622</td>\n",
       "      <td>0.760800</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.009046</td>\n",
       "      <td>1.709670</td>\n",
       "      <td>0.761667</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.008581</td>\n",
       "      <td>1.729421</td>\n",
       "      <td>0.764200</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>1.757223</td>\n",
       "      <td>0.765600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>1.763944</td>\n",
       "      <td>0.765600</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>1.794764</td>\n",
       "      <td>0.763867</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>1.791924</td>\n",
       "      <td>0.765067</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>1.814311</td>\n",
       "      <td>0.768267</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>1.812932</td>\n",
       "      <td>0.768400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>1.848005</td>\n",
       "      <td>0.766067</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>1.832108</td>\n",
       "      <td>0.767000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>1.846471</td>\n",
       "      <td>0.767333</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>1.859861</td>\n",
       "      <td>0.766533</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>1.857670</td>\n",
       "      <td>0.768733</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>1.859121</td>\n",
       "      <td>0.769933</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>1.860884</td>\n",
       "      <td>0.769400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>1.865785</td>\n",
       "      <td>0.768800</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>1.865472</td>\n",
       "      <td>0.769133</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>1.860447</td>\n",
       "      <td>0.769067</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>1.867444</td>\n",
       "      <td>0.768400</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(100,1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
